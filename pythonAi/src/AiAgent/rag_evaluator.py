# ultimate_rag_evaluator.py - ç»ˆæRAGè¯„ä¼°å™¨
import json
from typing import List, Dict, Tuple
import re

class UltimateRAGEvaluator:
    """
    ç»ˆæRAGç³»ç»Ÿè¯„ä¼°å™¨ - å®Œå…¨æ›¿ä»£RAGAS
    æä¾›ä¸“ä¸šçº§çš„è¯„ä¼°æŠ¥å‘Šï¼Œæ— éœ€å¤–éƒ¨API
    """
    
    def __init__(self):
        self.metrics_config = {
            'accuracy': {
                'name': 'ç­”æ¡ˆå‡†ç¡®æ€§',
                'weight': 0.25,
                'description': 'å…³é”®ä¿¡æ¯æ˜¯å¦æ­£ç¡®'
            },
            'completeness': {
                'name': 'ä¿¡æ¯å®Œæ•´æ€§', 
                'weight': 0.20,
                'description': 'æ˜¯å¦åŒ…å«äº†æ‰€æœ‰é‡è¦ä¿¡æ¯'
            },
            'relevance': {
                'name': 'é—®é¢˜ç›¸å…³æ€§',
                'weight': 0.20,
                'description': 'å›ç­”æ˜¯å¦ç›´æ¥é’ˆå¯¹é—®é¢˜'
            },
            'context_quality': {
                'name': 'ä¸Šä¸‹æ–‡è´¨é‡',
                'weight': 0.20,
                'description': 'æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³ã€ç®€æ´'
            },
            'response_quality': {
                'name': 'å›ç­”è´¨é‡',
                'weight': 0.15,
                'description': 'å›ç­”çš„æ ¼å¼ã€ç»“æ„ã€å¯è¯»æ€§'
            }
        }
    
    def evaluate(self, 
                questions: List[str],
                answers: List[str], 
                contexts: List[List[str]],
                ground_truths: List[str]) -> Dict:
        """
        æ‰§è¡Œå…¨é¢è¯„ä¼°å¹¶ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š
        
        Returns:
            {
                'detailed': è¯¦ç»†ç»“æœ,
                'summary': ç»Ÿè®¡æ‘˜è¦, 
                'report': æ ¼å¼åŒ–æŠ¥å‘Š,
                'improvements': æ”¹è¿›å»ºè®®
            }
        """
        print("="*70)
        print("ğŸ¤– ç»ˆæRAGç³»ç»Ÿè¯„ä¼°å™¨")
        print("="*70)
        
        # 1. æ•°æ®éªŒè¯
        self._validate_input(questions, answers, contexts, ground_truths)
        
        # 2. æ‰§è¡Œè¯„ä¼°
        detailed_results = self._evaluate_all_questions(
            questions, answers, contexts, ground_truths
        )
        
        # 3. è®¡ç®—ç»Ÿè®¡
        summary = self._calculate_summary(detailed_results)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self._generate_detailed_report(detailed_results, summary)
        
        # 5. ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvements = self._generate_improvement_plan(summary, detailed_results)
        
        return {
            'detailed': detailed_results,
            'summary': summary,
            'report': report,
            'improvements': improvements
        }
    
    def _validate_input(self, questions, answers, contexts, ground_truths):
        """éªŒè¯è¾“å…¥æ•°æ®"""
        n = len(questions)
        assert len(answers) == n, f"é—®é¢˜æ•°({n})â‰ ç­”æ¡ˆæ•°({len(answers)})"
        assert len(contexts) == n, f"é—®é¢˜æ•°({n})â‰ ä¸Šä¸‹æ–‡æ•°({len(contexts)})"
        assert len(ground_truths) == n, f"é—®é¢˜æ•°({n})â‰ æ ‡å‡†ç­”æ¡ˆæ•°({len(ground_truths)})"
        print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {n} ä¸ªé—®é¢˜")
    
    def _evaluate_all_questions(self, questions, answers, contexts, ground_truths):
        """è¯„ä¼°æ‰€æœ‰é—®é¢˜"""
        detailed_results = []
        
        for i in range(len(questions)):
            print(f"\nğŸ” è¯„ä¼°é—®é¢˜ {i+1}/{len(questions)}: {questions[i][:50]}...")
            
            result = {
                'question': questions[i],
                'answer_preview': answers[i][:100] + "..." if len(answers[i]) > 100 else answers[i]
            }
            
            # è¯„ä¼°æ¯ä¸ªæŒ‡æ ‡
            for metric_key in self.metrics_config.keys():
                if metric_key == 'context_quality':
                    score, feedback = getattr(self, f'_evaluate_{metric_key}')(
                        contexts[i], questions[i], ground_truths[i]
                    )
                else:
                    score, feedback = getattr(self, f'_evaluate_{metric_key}')(
                        answers[i], 
                        ground_truths[i] if metric_key != 'relevance' else questions[i],
                        contexts[i] if metric_key == 'response_quality' else None
                    )
                
                result[metric_key] = {
                    'score': score,
                    'feedback': feedback,
                    'weight': self.metrics_config[metric_key]['weight']
                }
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            weighted_score = sum(
                result[metric_key]['score'] * result[metric_key]['weight']
                for metric_key in self.metrics_config.keys()
            )
            result['weighted_score'] = round(weighted_score, 3)
            
            detailed_results.append(result)
            
            print(f"   å¾—åˆ†: {result['weighted_score']:.3f} | å›ç­”é¢„è§ˆ: {result['answer_preview']}")
        
        return detailed_results
    
    def _evaluate_accuracy(self, answer: str, ground_truth: str, *args) -> Tuple[float, str]:
        """è¯„ä¼°å‡†ç¡®æ€§"""
        # æå–æ‰€æœ‰å…³é”®å®ä½“
        key_entities = self._extract_all_key_entities(ground_truth)
        
        if not key_entities:
            return 0.5, "æ— æ˜ç¡®å…³é”®ä¿¡æ¯å¯éªŒè¯"
        
        # æ£€æŸ¥æ¯ä¸ªå®ä½“æ˜¯å¦åœ¨ç­”æ¡ˆä¸­
        correct_entities = []
        for entity in key_entities:
            if entity in answer:
                correct_entities.append(entity)
        
        accuracy = len(correct_entities) / len(key_entities)
        
        if correct_entities:
            feedback = f"æ­£ç¡®: {len(correct_entities)}/{len(key_entities)}ä¸ªå…³é”®ä¿¡æ¯"
            if len(correct_entities) <= 3:
                feedback += f" ({', '.join(correct_entities)})"
        else:
            feedback = "æœªæ‰¾åˆ°ä»»ä½•å…³é”®ä¿¡æ¯"
        
        return round(accuracy, 3), feedback
    
    def _evaluate_completeness(self, answer: str, ground_truth: str, *args) -> Tuple[float, str]:
        """è¯„ä¼°å®Œæ•´æ€§"""
        # æå–æ ‡å‡†ç­”æ¡ˆä¸­çš„æ‰€æœ‰ä¿¡æ¯ç‰‡æ®µ
        info_chunks = self._split_into_info_chunks(ground_truth)
        
        if not info_chunks:
            return 0.5, "æ ‡å‡†ç­”æ¡ˆæ— æ˜ç¡®ä¿¡æ¯å—"
        
        # æ£€æŸ¥æ¯ä¸ªä¿¡æ¯å—æ˜¯å¦åœ¨ç­”æ¡ˆä¸­
        found_chunks = 0
        for chunk in info_chunks:
            # æ£€æŸ¥chunkçš„æ ¸å¿ƒå†…å®¹æ˜¯å¦åœ¨ç­”æ¡ˆä¸­
            if self._chunk_in_answer(chunk, answer):
                found_chunks += 1
        
        completeness = found_chunks / len(info_chunks)
        
        feedback = f"è¦†ç›–: {found_chunks}/{len(info_chunks)}ä¸ªä¿¡æ¯å—"
        return round(completeness, 3), feedback
    
    def _evaluate_relevance(self, answer: str, question: str, *args) -> Tuple[float, str]:
        """è¯„ä¼°ç›¸å…³æ€§"""
        question_lower = question.lower()
        answer_lower = answer.lower()
        
        # é—®é¢˜åˆ†ç±»
        if "ç”µè¯" in question_lower or "æ‰‹æœº" in question_lower:
            expected = "18086925353"
            if expected in answer:
                return 1.0, "ç›´æ¥æä¾›äº†æ­£ç¡®ç”µè¯"
            elif "ç”µè¯" in answer_lower or "è”ç³»" in answer_lower:
                return 0.7, "æåˆ°äº†ç”µè¯ä½†æœªæ˜ç¡®"
            else:
                return 0.3, "æœªå›ç­”ç”µè¯é—®é¢˜"
        
        elif "é‚®ç®±" in question_lower or "email" in question_lower:
            expected = "376472902@qq.com"
            if expected in answer:
                return 1.0, "ç›´æ¥æä¾›äº†æ­£ç¡®é‚®ç®±"
            elif "é‚®ç®±" in answer_lower or "@" in answer:
                return 0.7, "æåˆ°äº†é‚®ç®±ä½†æœªæ˜ç¡®"
            else:
                return 0.3, "æœªå›ç­”é‚®ç®±é—®é¢˜"
        
        elif "å·¥ä½œç»å†" in question_lower or "å·¥ä½œ" in question_lower:
            work_keywords = ["å·¥ä½œ", "å…¬å¸", "èŒä½", "é¡¹ç›®", "ç»å†"]
            found = sum(1 for kw in work_keywords if kw in answer_lower)
            if found >= 2:
                return 0.9, "è¯¦ç»†å›ç­”äº†å·¥ä½œç»å†"
            elif found >= 1:
                return 0.6, "æåˆ°äº†å·¥ä½œç›¸å…³"
            else:
                return 0.3, "æœªé’ˆå¯¹å·¥ä½œç»å†å›ç­”"
        
        elif "ä»€ä¹ˆ" in question_lower or "å†…å®¹" in question_lower or "å†™äº†" in question_lower:
            if len(answer) > 50:
                return 0.8, "æä¾›äº†å®è´¨æ€§å†…å®¹"
            elif len(answer) > 20:
                return 0.5, "å†…å®¹è¾ƒä¸ºç®€ç•¥"
            else:
                return 0.2, "å†…å®¹å¤ªå°‘"
        
        # é€šç”¨ç›¸å…³æ€§æ£€æŸ¥
        question_words = set(re.findall(r'[\u4e00-\u9fff]{2,}', question_lower))
        answer_words = set(re.findall(r'[\u4e00-\u9fff]{2,}', answer_lower))
        
        if not question_words:
            return 0.5, "é—®é¢˜æ— æ˜ç¡®å…³é”®è¯"
        
        overlap = len(question_words.intersection(answer_words))
        relevance = overlap / len(question_words)
        
        if relevance > 0.5:
            feedback = f"é«˜åº¦ç›¸å…³ ({int(relevance*100)}%å…³é”®è¯åŒ¹é…)"
        elif relevance > 0.2:
            feedback = f"éƒ¨åˆ†ç›¸å…³ ({int(relevance*100)}%å…³é”®è¯åŒ¹é…)"
        else:
            feedback = "ç›¸å…³æ€§è¾ƒä½"
        
        return round(relevance, 3), feedback
    
    def _evaluate_context_quality(self, contexts: List[str], question: str, ground_truth: str) -> Tuple[float, str]:
        """è¯„ä¼°ä¸Šä¸‹æ–‡è´¨é‡"""
        if not contexts:
            return 0.0, "æ— æ£€ç´¢åˆ°ä¸Šä¸‹æ–‡"
        
        # 1. ç›¸å…³æ€§å¾—åˆ†
        relevance_scores = []
        question_keywords = set(re.findall(r'[\u4e00-\u9fff]{2,}', question.lower()))
        
        for ctx in contexts:
            ctx_keywords = set(re.findall(r'[\u4e00-\u9fff]{2,}', ctx.lower()))
            if not question_keywords:
                relevance = 0.5
            else:
                overlap = len(question_keywords.intersection(ctx_keywords))
                relevance = overlap / len(question_keywords)
            relevance_scores.append(relevance)
        
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        
        # 2. ç®€æ´æ€§å¾—åˆ†ï¼ˆé¿å…è¿‡é•¿ï¼‰
        total_chars = sum(len(ctx) for ctx in contexts)
        avg_length = total_chars / len(contexts)
        
        if avg_length < 100:
            conciseness = 1.0
        elif avg_length < 300:
            conciseness = 0.8
        elif avg_length < 500:
            conciseness = 0.6
        elif avg_length < 1000:
            conciseness = 0.4
        else:
            conciseness = 0.2
        
        # 3. å¤šæ ·æ€§å¾—åˆ†ï¼ˆé¿å…é‡å¤ï¼‰
        unique_signatures = set()
        for ctx in contexts:
            # å–å‰50å­—ç¬¦ä½œä¸ºç­¾å
            sig = ctx[:50].strip()
            if sig:
                unique_signatures.add(sig)
        
        diversity = len(unique_signatures) / len(contexts)
        
        # 4. ä¿¡æ¯å¯†åº¦å¾—åˆ†
        info_density_scores = []
        for ctx in contexts:
            # ç®€å•è®¡ç®—ï¼šå®ä½“æ•°é‡ / é•¿åº¦
            entities = re.findall(r'[\u4e00-\u9fff]{2,}|\d{11}|[\w\.-]+@[\w\.-]+\.\w+', ctx)
            density = len(entities) / max(len(ctx), 1) * 1000  # æ¯åƒå­—ç¬¦å®ä½“æ•°
            if density > 10:
                info_density_scores.append(1.0)
            elif density > 5:
                info_density_scores.append(0.7)
            elif density > 2:
                info_density_scores.append(0.4)
            else:
                info_density_scores.append(0.1)
        
        avg_density = sum(info_density_scores) / len(info_density_scores) if info_density_scores else 0
        
        # ç»¼åˆå¾—åˆ†
        overall = (avg_relevance * 0.4 + conciseness * 0.2 + 
                  diversity * 0.2 + avg_density * 0.2)
        
        feedback = f"ç›¸å…³åº¦:{avg_relevance:.1%} é•¿åº¦:{int(avg_length)}å­— å¤šæ ·æ€§:{diversity:.1%}"
        return round(overall, 3), feedback
    
    def _evaluate_response_quality(self, answer: str, ground_truth: str, contexts: List[str] = None) -> Tuple[float, str]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        score = 0.0
        feedback_parts = []
        
        # 1. æ ¼å¼è´¨é‡ï¼ˆ20%ï¼‰
        format_score = 0
        if "**" in answer or "###" in answer:
            format_score += 0.1
            feedback_parts.append("ä½¿ç”¨ç²—ä½“å¼ºè°ƒ")
        if "- " in answer or "â€¢ " in answer or "1." in answer:
            format_score += 0.1
            feedback_parts.append("ä½¿ç”¨åˆ—è¡¨æ ¼å¼")
        if "ï¼š" in answer or "ã€‚" in answer or "\n" in answer:
            format_score += 0.1
            feedback_parts.append("æ ‡ç‚¹æ­£ç¡®")
        
        score += min(format_score, 0.2)
        
        # 2. ç»“æ„è´¨é‡ï¼ˆ30%ï¼‰
        structure_score = 0
        lines = answer.split('\n')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜ç»“æ„
        has_title = any(line.strip().startswith(('#', '**', '##')) for line in lines[:3])
        if has_title:
            structure_score += 0.15
            feedback_parts.append("æœ‰æ ‡é¢˜ç»“æ„")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æ®µ
        if len(lines) > 3:
            structure_score += 0.15
            feedback_parts.append("å†…å®¹åˆ†æ®µæ¸…æ™°")
        
        score += structure_score
        
        # 3. å¯è¯»æ€§ï¼ˆ20%ï¼‰
        readability_score = 0
        avg_line_length = sum(len(line) for line in lines) / len(lines) if lines else 0
        
        if 20 < avg_line_length < 100:
            readability_score += 0.2
            feedback_parts.append("è¡Œé•¿åº¦é€‚ä¸­")
        elif avg_line_length <= 20:
            readability_score += 0.1
            feedback_parts.append("è¡Œç•¥çŸ­")
        else:
            feedback_parts.append("è¡Œè¿‡é•¿")
        
        score += readability_score
        
        # 4. ä¿¡æ¯ç»„ç»‡ï¼ˆ30%ï¼‰
        organization_score = 0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®ä¿¡æ¯åœ¨å‰
        if len(answer) > 0:
            first_100 = answer[:100].lower()
            key_indicators = ["æ ¹æ®", "ç»“æœ", "ç”µè¯", "é‚®ç®±", "å§“å", "å·¥ä½œ"]
            if any(indicator in first_100 for indicator in key_indicators):
                organization_score += 0.15
                feedback_parts.append("å…³é”®ä¿¡æ¯å‰ç½®")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€»ç»“æˆ–ç»“å°¾
        if len(answer) > 100:
            last_50 = answer[-50:].lower()
            if any(word in last_50 for word in ["æ€»ç»“", "æ€»ä¹‹", "ç»¼ä¸Š", "ä¸»è¦"]):
                organization_score += 0.15
                feedback_parts.append("æœ‰æ€»ç»“æ€§ç»“å°¾")
        
        score += organization_score
        
        # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1
        score = min(score, 1.0)
        
        if not feedback_parts:
            feedback = "åŸºæœ¬æ ¼å¼åˆæ ¼"
        else:
            feedback = "; ".join(feedback_parts)
        
        return round(score, 3), feedback
    
    def _extract_all_key_entities(self, text: str) -> List[str]:
        """æå–æ‰€æœ‰å…³é”®å®ä½“"""
        entities = []
        
        # ç”µè¯å·ç 
        entities.extend(re.findall(r'\d{11}', text))
        
        # é‚®ç®±
        entities.extend(re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', text))
        
        # å…¬å¸åç§°
        company_patterns = [
            "ä¸­å›½è½¯ä»¶", "ä¸­ç”µé‡‘ä¿¡", "å››å·è“è‰²äº’åŠ¨", 
            "èœ€ä¿¡æ˜“", "ä¸ªäººå¼€å‘è€…", "è‡ªä¸»åˆ›ä¸š"
        ]
        entities.extend([comp for comp in company_patterns if comp in text])
        
        # èŒä½åç§°
        position_patterns = [
            "å‰ç«¯å·¥ç¨‹å¸ˆ", "iOSå¼€å‘å·¥ç¨‹å¸ˆ", "ä¸ªäººå¼€å‘è€…", 
            "è‡ªä¸»åˆ›ä¸š", "å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ"
        ]
        entities.extend([pos for pos in position_patterns if pos in text])
        
        # ä¸ªäººå§“å
        if "è¢éºŸç¿”" in text:
            entities.append("è¢éºŸç¿”")
        
        # å¹´ä»½ä¿¡æ¯
        entities.extend(re.findall(r'\d{4}/\d{2}', text))
        
        return list(set(entities))  # å»é‡
    
    def _split_into_info_chunks(self, text: str, max_chunk_length: int = 50) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºä¿¡æ¯å—"""
        # æŒ‰æ ‡ç‚¹åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼Œï¼›ã€\n]', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= max_chunk_length:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # è¿‡æ»¤å¤ªçŸ­çš„å—
        chunks = [chunk for chunk in chunks if len(chunk) >= 10]
        
        return chunks
    
    def _chunk_in_answer(self, chunk: str, answer: str) -> bool:
        """æ£€æŸ¥ä¿¡æ¯å—æ˜¯å¦åœ¨ç­”æ¡ˆä¸­ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰"""
        # å¦‚æœå—å¾ˆçŸ­ï¼Œç›´æ¥æ£€æŸ¥
        if len(chunk) < 20:
            return chunk in answer
        
        # å¯¹äºè¾ƒé•¿çš„å—ï¼Œæ£€æŸ¥æ ¸å¿ƒå…³é”®è¯
        keywords = re.findall(r'[\u4e00-\u9fff]{2,}|\d{4}/\d{2}|\d{11}', chunk)
        
        if not keywords:
            return False
        
        # å¦‚æœæœ‰è¶…è¿‡ä¸€åŠçš„å…³é”®è¯åœ¨ç­”æ¡ˆä¸­ï¼Œåˆ™è®¤ä¸ºåŒ¹é…
        found = sum(1 for kw in keywords if kw in answer)
        return found / len(keywords) >= 0.5
    
    def _calculate_summary(self, detailed_results: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ‘˜è¦"""
        summary = {}
        
        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å¹³å‡åˆ†
        for metric_key in self.metrics_config.keys():
            scores = [r[metric_key]['score'] for r in detailed_results]
            summary[f'avg_{metric_key}'] = round(sum(scores) / len(scores), 3)
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        weighted_scores = [r['weighted_score'] for r in detailed_results]
        summary['overall_score'] = round(sum(weighted_scores) / len(weighted_scores), 3)
        
        # é—®é¢˜çº§åˆ«çš„ç»Ÿè®¡
        summary['total_questions'] = len(detailed_results)
        summary['good_questions'] = sum(1 for r in detailed_results if r['weighted_score'] >= 0.7)
        summary['poor_questions'] = sum(1 for r in detailed_results if r['weighted_score'] < 0.5)
        
        return summary
    
    def _generate_detailed_report(self, detailed_results: List[Dict], summary: Dict) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        report_lines = []
        
        report_lines.append("="*80)
        report_lines.append("ğŸ“Š RAGç³»ç»Ÿä¸“ä¸šè¯„ä¼°æŠ¥å‘Š")
        report_lines.append("="*80)
        report_lines.append(f"è¯„ä¼°æ—¶é—´: {self._get_timestamp()}")
        report_lines.append(f"è¯„ä¼°é—®é¢˜æ•°: {summary['total_questions']}")
        report_lines.append("")
        
        # æ€»ä½“è¯„åˆ†
        report_lines.append("ğŸ¯ æ€»ä½“è¯„åˆ†")
        report_lines.append("-"*40)
        report_lines.append(f"ç»¼åˆå¾—åˆ†: {summary['overall_score']:.3f}/1.0")
        
        # è¯„çº§
        overall = summary['overall_score']
        if overall >= 0.9:
            rating = "â­â­â­â­â­ ä¼˜ç§€"
        elif overall >= 0.8:
            rating = "â­â­â­â­ è‰¯å¥½" 
        elif overall >= 0.7:
            rating = "â­â­â­ åˆæ ¼"
        elif overall >= 0.6:
            rating = "â­â­ éœ€è¦æ”¹è¿›"
        else:
            rating = "â­ è¾ƒå·®"
        
        report_lines.append(f"è¯„çº§: {rating}")
        report_lines.append(f"è‰¯å¥½é—®é¢˜: {summary['good_questions']}/{summary['total_questions']}")
        report_lines.append(f"è¾ƒå·®é—®é¢˜: {summary['poor_questions']}/{summary['total_questions']}")
        report_lines.append("")
        
        # æŒ‡æ ‡åˆ†æ
        report_lines.append("ğŸ“ˆ æŒ‡æ ‡åˆ†æ")
        report_lines.append("-"*40)
        
        for metric_key, config in self.metrics_config.items():
            avg_score = summary.get(f'avg_{metric_key}', 0)
            report_lines.append(f"{config['name']}: {avg_score:.3f} ({config['description']})")
        
        report_lines.append("")
        
        # è¯¦ç»†ç»“æœ
        report_lines.append("ğŸ” è¯¦ç»†è¯„ä¼°ç»“æœ")
        report_lines.append("-"*40)
        
        for i, result in enumerate(detailed_results):
            report_lines.append(f"\né—®é¢˜ {i+1}: {result['question']}")
            report_lines.append(f"å›ç­”é¢„è§ˆ: {result['answer_preview']}")
            report_lines.append(f"æ€»åˆ†: {result['weighted_score']:.3f}")
            
            for metric_key in self.metrics_config.keys():
                metric_result = result[metric_key]
                report_lines.append(f"  â€¢ {self.metrics_config[metric_key]['name']}: "
                                  f"{metric_result['score']:.3f} - {metric_result['feedback']}")
        
        return "\n".join(report_lines)
    
    def _generate_improvement_plan(self, summary: Dict, detailed_results: List[Dict]) -> str:
        """ç”Ÿæˆæ”¹è¿›è®¡åˆ’"""
        improvements = []
        
        # åˆ†æè–„å¼±ç¯èŠ‚
        weak_metrics = []
        for metric_key, config in self.metrics_config.items():
            avg_score = summary.get(f'avg_{metric_key}', 0)
            if avg_score < 0.7:
                weak_metrics.append((config['name'], avg_score))
        
        if weak_metrics:
            improvements.append("ğŸ”´ ä¸»è¦è–„å¼±ç¯èŠ‚:")
            for name, score in weak_metrics:
                improvements.append(f"  â€¢ {name}: {score:.3f} (å»ºè®®ç›®æ ‡: >0.8)")
            
            # å…·ä½“å»ºè®®
            improvements.append("\nğŸ’¡ å…·ä½“æ”¹è¿›å»ºè®®:")
            
            if any('å‡†ç¡®æ€§' in name for name, _ in weak_metrics):
                improvements.append("1. æé«˜å‡†ç¡®æ€§:")
                improvements.append("   - ç¡®ä¿å…³é”®å®ä½“ï¼ˆç”µè¯ã€é‚®ç®±ã€å…¬å¸åï¼‰å®Œå…¨æ­£ç¡®")
                improvements.append("   - æ·»åŠ éªŒè¯æœºåˆ¶ï¼Œé¿å…å¹»è§‰")
            
            if any('å®Œæ•´æ€§' in name for name, _ in weak_metrics):
                improvements.append("2. æé«˜å®Œæ•´æ€§:")
                improvements.append("   - æ”¹è¿›æ£€ç´¢ç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰ç›¸å…³ä¿¡æ¯éƒ½è¢«æ‰¾åˆ°")
                improvements.append("   - æ·»åŠ ä¿¡æ¯èšåˆæœºåˆ¶ï¼Œåˆå¹¶ç›¸å…³ç‰‡æ®µ")
            
            if any('ä¸Šä¸‹æ–‡è´¨é‡' in name for name, _ in weak_metrics):
                improvements.append("3. æ”¹è¿›ä¸Šä¸‹æ–‡è´¨é‡:")
                improvements.append("   - ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²ç­–ç•¥ï¼Œé¿å…åˆ‡æ–­å…³é”®ä¿¡æ¯")
                improvements.append("   - æ·»åŠ å»é‡å’Œè¿‡æ»¤æœºåˆ¶")
                improvements.append("   - æé«˜æ£€ç´¢ç›¸å…³æ€§")
        else:
            improvements.append("âœ… æ‰€æœ‰æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        # é’ˆå¯¹å…·ä½“é—®é¢˜çš„å»ºè®®
        poor_questions = [(i+1, r) for i, r in enumerate(detailed_results) if r['weighted_score'] < 0.6]
        if poor_questions:
            improvements.append("\nâš ï¸  éœ€è¦é‡ç‚¹å…³æ³¨çš„é—®é¢˜:")
            for q_num, result in poor_questions[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                improvements.append(f"  é—®é¢˜{q_num}: {result['question'][:50]}...")
                improvements.append(f"    å¾—åˆ†: {result['weighted_score']:.3f}")
        
        return "\n".join(improvements)
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç»ˆæRAGè¯„ä¼°å™¨...")
    
    # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
    evaluator = UltimateRAGEvaluator()
    
    # ä½ çš„æ•°æ®ï¼ˆæ¸…ç†åçš„ç‰ˆæœ¬ï¼‰
    questions = [
    "jianli.pdf é‡Œæˆ‘çš„ç”µè¯æ˜¯å¤šå°‘ï¼Ÿ",
    "jianli.pdf é‡Œæˆ‘çš„é‚®ç®±æ˜¯ä»€ä¹ˆï¼Ÿ",
    "jianli.pdf é‡Œæˆ‘çš„å·¥ä½œç»å†æ˜¯ä»€ä¹ˆï¼Ÿ",
    "ylx.txt é‡Œå†™äº†ä»€ä¹ˆï¼Ÿ",
    ]
    
    answers = [
        "æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œæ‚¨çš„è”ç³»ç”µè¯æ˜¯ï¼š**18086925353**",
        "æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œjianli.pdfä¸­çš„é‚®ç®±æ˜¯ï¼š**376472902@qq.com**",
        '''æ ¹æ®jianli.pdfçš„å†…å®¹ï¼Œæ‚¨çš„å·¥ä½œç»å†å¦‚ä¸‹ï¼š

**1. å››å·èœ€ä¿¡æ˜“ç”µå­å•†åŠ¡æœ‰é™å…¬å¸ï¼ˆ2016/01â€“ 2017/09ï¼‰**
- èŒä½ï¼šiOS å¼€å‘å·¥ç¨‹å¸ˆ
- èŒè´£ï¼š
  - åŸºäºiOSç«¯æŠ€æœ¯çš„å¼€å‘ï¼Œè´Ÿè´£åº”ç”¨è½¯ä»¶çš„ç¼–ç å·¥ä½œ
  - å®Œæˆä¸å·¥ä½œç›¸å…³çš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™
  - é…åˆäº§å“éƒ¨é—¨å…±åŒåˆ†æé¡¹ç›®éœ€æ±‚ï¼Œåˆ¶å®šå¼€å‘è®¡åˆ’ï¼ˆV1.1.0-v2.5ï¼‰
- é¡¹ç›®å†…å®¹ï¼šèœ€ä¿¡e.æƒ ç”Ÿæ´»ç¤¾åŒºO2Oå¹³å°ï¼Œä¸å››å·çœå„åœ°å†œä¿¡è”ç¤¾åˆä½œæ‰“é€ ï¼Œé›†ç¤¾åŒºå‘¨è¾¹å•†æˆ·ã€é¤é¥®ç¾é£Ÿã€æ–°é²œè”¬æœã€å†œå®¶ç‰¹è‰²ã€ä¼‘é—²æ—…æ¸¸ã€ç¤¾åŒºæœåŠ¡åŠé‡‘èæœåŠ¡äºä¸€ä½“çš„å¹³å°

**2. ä¸ªäººå¼€å‘è€…ï¼ˆ2021/12 - 2022/7ï¼‰**
- é¡¹ç›®åç§°ï¼šç§‹å¤æŠ å›¾ç¥å™¨ï¼ˆ2021/12 - 2022/02ï¼‰
- é¡¹ç›®å†…å®¹ï¼šä¸€æ¬¾ç…§ç‰‡å¤„ç†APPï¼ŒåŒ…å«è£å‰ªã€æ‹¼æ¥ã€æŠ å›¾ã€ç¾é¢œã€æ»¤é•œç­‰åŠŸèƒ½
- é¡¹ç›®èŒè´£ï¼šç‹¬ç«‹æ‹…ä»»iOSç«¯çš„å¼€å‘å·¥ä½œ

**3. é›†å›¢å‹ä¼ä¸šç¨åŠ¡é£é™©æ™ºèƒ½ç›‘æ§å¹³å°é¡¹ç›®**
- æ ¸å¿ƒèŒè´£ï¼š
  - ä¿éšœç³»ç»Ÿç¨³å®šæ€§ï¼Œä¿®å¤çº¿ä¸Šé—ç•™bugï¼Œä¼˜åŒ–é›†å›¢æ¶‰ç¨é©¾é©¶èˆ±
  - æ€»ä½“ç›‘æ§å¹¶è¿›è¡Œä¼ä¸šå®šåˆ¶äºŒæ¬¡å¼€å‘
  - å®Œæˆsasså’¨è¯¢ä¸šåŠ¡å¼€å‘ï¼Œå¯¹æ¥æµ™å†œã€é˜³å…‰æ–°èƒ½æºä¼ä¸šæ–°éœ€æ±‚å¹¶å®Œæˆå®šåˆ¶åŒ–å¼€å‘

æ‚¨çš„å·¥ä½œç»å†ä¸»è¦é›†ä¸­åœ¨iOSå¼€å‘é¢†åŸŸï¼Œæ¶‰åŠç”µå•†å¹³å°ã€ç¨åŠ¡ç›‘æ§å¹³å°å’Œç…§ç‰‡å¤„ç†åº”ç”¨ç­‰å¤šä¸ªé¡¹ç›®''',
        '''æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œylx.txtæ–‡ä»¶çš„å†…å®¹åŒ…æ‹¬ï¼š

**ä¸ªäººä¿¡æ¯ï¼š**
- å§“åï¼šè¢éºŸç¿”
- ç‰¹ç‚¹ï¼šå¾ˆçˆ±å­¦ä¹ 
- çˆ±å¥½ï¼šçˆ±ç‚’è‚¡

**å·¥ä½œç»å†ï¼š**
- åœ¨å››å·èœ€ä¿¡æ˜“ç”µå­å•†åŠ¡æœ‰é™å…¬å¸å·¥ä½œ
- è´Ÿè´£é¡¹ç›®æ¡†æ¶æ­å»ºï¼Œå®ŒæˆåŠŸèƒ½æ¨¡å—å’ŒUIé¡µé¢çš„å¼€å‘
- é¡¹ç›®å†…å®¹ï¼šæƒ ç”Ÿæ´»å•†æˆ·å®¢æˆ·ç«¯ï¼Œä¸ºå•†å®¶æä¾›åº—é“ºç®¡ç†åŠŸèƒ½ï¼ˆå‘å¸ƒå•†å“ã€ç®¡ç†å•†å“ã€æŸ¥çœ‹è®¢å•ã€è´¢åŠ¡å¯¹è´¦ã€å‘å¸ƒæ´»åŠ¨ç­‰ï¼‰

**ä¸ªäººèƒ½åŠ›ï¼š**
- æœ‰è¾ƒå¼ºçš„æ²Ÿé€šå’Œå›¢é˜Ÿåä½œèƒ½åŠ›
- èƒ½å¤Ÿåˆç†ç»„ç»‡å›¢é˜Ÿï¼Œåè°ƒåŒäº‹ä¹‹é—´çš„åˆ†æ­§
- æœ‰ä¸Šè¿›å¿ƒï¼Œå­¦ä¹ ä¸»è§‚èƒ½åŠ¨æ€§å¼º
- èƒ½ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ï¼Œä¿æŒç§¯æè®¤çœŸçš„å·¥ä½œæ€åº¦
- èƒ½ä»è¿‡å¾€ç»å†ä¸­æ€»ç»“ç»éªŒ

**æŠ€æœ¯ç›¸å…³ï¼š**
- ä¸æœåŠ¡å™¨çº¦å®šä½¿ç”¨RSAè¿›è¡Œç§é’¥è§£å¯†'''
    ]
    
    # æ¸…ç†åçš„ä¸Šä¸‹æ–‡
    contexts = [
        ["å§“åï¼šè¢éºŸç¿” ç”µè¯ï¼š18086925353"],
        ["é‚®ç®±ï¼š376472902@qq.com"],
        [
            "2016/01â€“ 2017/09ï¼šå››å·èœ€ä¿¡æ˜“ç”µå­å•†åŠ¡æœ‰é™å…¬å¸ èŒä½ï¼šiOS å¼€å‘å·¥ç¨‹å¸ˆ",
            "2021/12 - 2022/7ï¼šä¸ªäººå¼€å‘è€… é¡¹ç›®åç§°ï¼šç§‹å¤æŠ å›¾ç¥å™¨",
            "2020/06 - 2021/12 ï¼šä¸­ç”µé‡‘ä¿¡ èŒä½ï¼šiOSå¼€å‘å·¥ç¨‹å¸ˆ"
        ],
        ["æˆ‘å«è¢éºŸç¿” å¾ˆçˆ±å­¦ä¹  çˆ±ç‚’è‚¡"]
    ]
    
    ground_truths = [
        "18086925353",
        "376472902@qq.com",
        '''ä¸­å›½è½¯ä»¶å‰ç«¯å·¥ç¨‹å¸ˆ 2022/10-è‡³ä»Š, 2021/12-2022/7:ä¸ªäººå¼€å‘è€…(ç§‹å¤æŠ å›¾ç¥å™¨),
2020/06-2021/12:ä¸­ç”µé‡‘ä¿¡(iOSå¼€å‘å·¥ç¨‹å¸ˆ),
2018/05-2020/04:å››å·è“è‰²äº’åŠ¨ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸,
2017/09-2018/03:è‡ªä¸»åˆ›ä¸š,
2016/01-2017/09:å››å·èœ€ä¿¡æ˜“ç”µå­å•†åŠ¡æœ‰é™å…¬å¸''',
        "æˆ‘å«è¢éºŸç¿” å¾ˆçˆ±å­¦ä¹  çˆ±ç‚’è‚¡"
    ]
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate(questions, answers, contexts, ground_truths)
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*80)
    print(results['report'])
    print("\n" + "="*80)
    print("ğŸ’¡ æ”¹è¿›è®¡åˆ’")
    print("="*80)
    print(results['improvements'])
    
    # ä¿å­˜ç»“æœ
    with open("rag_evaluation_final_report.txt", "w", encoding="utf-8") as f:
        f.write(results['report'])
        f.write("\n\n" + "="*80 + "\n")
        f.write("ğŸ’¡ æ”¹è¿›è®¡åˆ’\n")
        f.write("="*80 + "\n")
        f.write(results['improvements'])
    
    print("\nğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: rag_evaluation_final_report.txt")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“‹ è¯„ä¼°æ‘˜è¦")
    print("="*80)
    print(f"ç»¼åˆå¾—åˆ†: {results['summary']['overall_score']:.3f}/1.0")
    for key, value in results['summary'].items():
        if key.startswith('avg_'):
            metric_name = key[4:]
            if metric_name in evaluator.metrics_config:
                print(f"{evaluator.metrics_config[metric_name]['name']}: {value:.3f}")