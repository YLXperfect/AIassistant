# lab_rag.py - æ­¥éª¤1: æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os

#æ–‡ä»¶è·¯å¾„  ç»å¯¹è·¯å¾„
# DOCUMENT_PATH = "/Users/eval/Desktop/é¡¹ç›®/é¡¹ç›®/Python AIé¡¹ç›®/Aiassistant/AIassistant/pythonAi/src/AiAgent/ylx.txt"   
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºå®Œæ•´è·¯å¾„
DOCUMENT_PATH = os.path.join(current_dir, "ylx.txt")



def load_and_split_documents():
    """åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£"""
    try:
        # 1. åŠ è½½æ–‡æ¡£ï¼ˆå°è¯•utf-8ï¼Œå¤±è´¥åˆ™ç”¨gbkï¼‰
        try:
            loader = TextLoader(DOCUMENT_PATH, encoding="utf-8")
        except:
            loader = TextLoader(DOCUMENT_PATH, encoding="gbk")
        
        documents = loader.load()
        print(f"âœ… å·²åŠ è½½æ–‡æ¡£ï¼ŒåŸå§‹é¡µæ•°: {len(documents)}")
        print(f"   ç¬¬ä¸€é¡µé¢„è§ˆ: {documents[0].page_content[:200]}...\n")
        
        # 2. åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        split_docs = text_splitter.split_documents(documents)
        print(f"âœ… æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(split_docs)} ä¸ªç‰‡æ®µã€‚")
        if split_docs:
            print(f"   ç‰‡æ®µç¤ºä¾‹ (1/{len(split_docs)}): {split_docs[0].page_content[:200]}...\n")
        return split_docs
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {DOCUMENT_PATH}")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None


#  - æ­¥éª¤2: å‘é‡åŒ–ä¸å­˜å‚¨
from langchain_chroma import Chroma
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain import zhipuai
def create_vector_store(split_docs):
    """å°†æ–‡æ¡£ç‰‡æ®µå‘é‡åŒ–å¹¶å­˜å‚¨åˆ°ChromaDB"""
    print("\n=== ç¬¬äºŒæ­¥ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“ ===")
    
    # 1. åˆå§‹åŒ–æ™ºè°±åµŒå…¥æ¨¡å‹
    # æ³¨æ„ï¼šæ¨¡å‹åå¯èƒ½éœ€è¦æ ¹æ®æ™ºè°±æœ€æ–°æ–‡æ¡£è°ƒæ•´
    try:
        embeddings = ZhipuAIEmbeddings(
            api_key=os.getenv("ZHIPUAI_API_KEY"),  # å¤ç”¨ä½ çš„å¯¹è¯æ¨¡å‹å¯†é’¥
            model="embedding-2"  # æˆ– "text_embedding"ï¼Œè¯·ç¡®è®¤
        )
        print("âœ… åµŒå…¥æ¨¡å‹å·²åˆå§‹åŒ–")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   å¯èƒ½åŸå› ï¼š1) API Keyæ— æƒé™ 2) æ¨¡å‹åé”™è¯¯ 3) ç½‘ç»œé—®é¢˜")
        return None
    
    # 2. åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆæŒä¹…åŒ–åˆ°æœ¬åœ°ï¼‰
    try:
        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory="./chroma_db"  # å‘é‡æ•°æ®åº“æœ¬åœ°ç›®å½•
        )
        print("âœ… å‘é‡æ•°æ®åº“å·²åˆ›å»ºå¹¶ä¿å­˜è‡³ ./chroma_db")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        return None
    
    # 3. æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    test_query = "Eval"  # ä½¿ç”¨ä½ æ–‡æ¡£ä¸­çš„å…³é”®è¯
    print(f"\nğŸ” æµ‹è¯•æ£€ç´¢: æŸ¥è¯¢ '{test_query}'")
    try:
        results = vectorstore.similarity_search(test_query, k=2)
        for i, doc in enumerate(results):
            print(f"  ç»“æœ {i+1}: {doc.page_content[:100]}...")
    except Exception as e:
        print(f"âŒ æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
    
    return vectorstore

# ä¿®æ”¹ä¸»å‡½æ•°ä»¥åŒ…å«ç¬¬äºŒæ­¥
if __name__ == "__main__":
    print("=== RAGç¬¬ä¸€æ­¥ï¼šæ–‡æ¡£åŠ è½½ä¸åˆ†å‰² ===")
    split_docs = load_and_split_documents()
    
    if split_docs:
        # æ–°å¢ï¼šæ‰§è¡Œç¬¬äºŒæ­¥
        vectorstore = create_vector_store(split_docs)
